{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17d24192-5802-4f93-8589-89d58249034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#modelo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, BatchNormalization, LeakyReLU, Conv2DTranspose, Conv2D, Dropout, Flatten, Reshape, Input, Lambda, Concatenate\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, median_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212721a-b6f6-4d0a-9fa1-8023635e2ce3",
   "metadata": {},
   "source": [
    "### RNN: Model\n",
    "#### Keras es una biblioteca de redes neuronales de alto nivel, escrita en Python y capaz de ejecutarse sobre TensorFlow\n",
    "Keras Sequential: para inicializar la red neuronal, crea el contenedor de la red lstm. Este modelo se refiere a que crearemos una serie de capas de neuronas secuenciales, “una delante de otra”.\n",
    "Dropout : para evitar el sobreajuste con capas de deserción (Especificar 0.2 en la capa Dropout significa que se eliminarán el 20% de las capas)\n",
    "Dense: para agregar una capa de red neuronal densamente conectada.\n",
    "\n",
    "La capa LSTM se agrega con los siguientes argumentos:\n",
    "-num:numero de neuronas\n",
    "-return_sequences: True es necesario para apilar capas LSTM, por lo que la capa LSTM consecuente tiene una entrada de secuencia tridimensional\n",
    "-input_shape: es la forma del entrenamiento conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4f538e-c766-4eac-864b-7206aa7433d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_RNN(num,epochs_rnn,batch_size_rnn,X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    '''La red tiene una capa visible con 1 entrada, 4 capas ocultas con 50 bloques o neuronas y una capa de\n",
    "       salida que hace la predicción de un valor único. La red está entrenada con 300 epocas y se utiliza \n",
    "       tamaño de lote de 32'''\n",
    "    \n",
    "    model = tf.keras.models.Sequential() #crea el contenedor de la red lstm\n",
    "    #first LSTM\n",
    "    model.add(tf.keras.layers.LSTM(units=num,return_sequences=True,input_shape= X_train.shape[1:])) # se agrega la red especificandoles el numero de neuronas:50 y el tamaño de la entrada:35,1\n",
    "    model.add(tf.keras.layers.Dropout(0.2)) # regularizacion en las conexiones de entrada\n",
    "    #second LSTM\n",
    "    model.add(tf.keras.layers.LSTM(units=num,return_sequences=True))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    #third LSTM\n",
    "    model.add(tf.keras.layers.LSTM(units=num,return_sequences=True))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    #fourth LSTM\n",
    "    model.add(tf.keras.layers.LSTM(num))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Output \n",
    "    model.add(tf.keras.layers.Dense(units=1)) #activation='relu'    # para la capa de salida usamos la funcion dense y especificamos que el dato de salida tendra tamaño igual a 1\n",
    "    \n",
    "    '''Para compilar nuestro modelo usamos el optimizador de Adam y establecemos la pérdida como mean_squared_error. \n",
    "    Después de eso, ajustamos el modelo para que se ejecute durante 300 épocas (las épocas son la cantidad de veces \n",
    "    que el algoritmo de aprendizaje funcionará en todo el conjunto de entrenamiento) con un tamaño de lote de 32.'''\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='mse', metrics=[tf.metrics.MeanAbsoluteError(),tf.metrics.MeanAbsolutePercentageError()])\n",
    "    \n",
    "    #definir \"early stopping\" la loss de validacion no mejora en X épocas\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=200,  #numero de epocas sin mejora\n",
    "                                                      restore_best_weights = True)\n",
    "    \n",
    "    # Se ajusta el modelo\n",
    "    history_rnn = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = batch_size_rnn, \n",
    "    verbose = 1,\n",
    "    epochs = epochs_rnn, \n",
    "    validation_data = (X_test, y_test)\n",
    "    ,callbacks=[early_stopping])\n",
    "    \n",
    "    # Función de perdida\n",
    "    print('Función de perdida de acuerdo al número de epocas')\n",
    "    plt.plot(history_rnn.history['loss'])\n",
    "    plt.show()\n",
    "    \n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188bb53e-2f37-4c24-a226-3beee81c53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df):\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    # Train - Test\n",
    "    data_train, data_test = train_test_split(data)\n",
    "    print('Shape data train:', data_train.shape) \n",
    "    print('Shape data test:', data_test.shape) \n",
    "    \n",
    "    # Normalizar datos (Reshape da forma a una matriz)\n",
    "    scaler, data_train_scaled, data_test_scaled = normalize(data_train.values.reshape (-1, 1), data_test.values.reshape (-1, 1))\n",
    "     \n",
    "    # Estructura data para input rnn\n",
    "    time_step_in = 35 # predecir un valor en el futuro a partir de 35 valores pasados\n",
    "    X_train, y_train = processData_3D_structure(data_train_scaled, time_step_in) \n",
    "    X_test, y_test= processData_3D_structure(data_test_scaled, time_step_in) \n",
    "    \n",
    "    print('\\nEstructura 3D : Input RNN')\n",
    "    print('Dimension X_train',X_train.shape )\n",
    "    print('Dimension y_train',y_train.shape )\n",
    "    print('\\nDimension X_test',X_test.shape )\n",
    "    print('Dimension y_test',y_test.shape )\n",
    "    \n",
    "    # model LSTM\n",
    "    \n",
    "    #Hiperparametros rnn\n",
    "    batch_size_rnn= 32 # es el número de muestras entre las actualizaciones de peso del modelo\n",
    "    epochs_rnn= 500 # repeticiones\n",
    "    num=50 # número de neuronas\n",
    "\n",
    "    print('\\nComenzando con el entrenamiento de la red..')\n",
    "    model=create_LSTM_RNN(num,epochs_rnn,batch_size_rnn,X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    print('\\nTermino del entrenamiento')\n",
    "    \n",
    "    # Estimamos el rendimiento del modelo para el conjunto de datos de entrenamiento y prueba\n",
    "    predict_train_rnn= model.predict(X_train)\n",
    "    y_train_predicted_rnn= scaler.inverse_transform(predict_train_rnn)\n",
    "    y_train_real=scaler.inverse_transform(y_train)\n",
    "\n",
    "    predict_test_rnn= model.predict(X_test)\n",
    "    y_test_predicted_rnn= scaler.inverse_transform(predict_test_rnn)\n",
    "    y_test_real=scaler.inverse_transform(y_test)\n",
    "\n",
    "    prueba_y_predicted=np.concatenate((y_train_predicted_rnn, y_test_predicted_rnn), axis=0, out=None)\n",
    "    prueba_y_real=np.concatenate((y_train_real, y_test_real), axis=0, out=None)\n",
    "    \n",
    "    print('Rendimiento del modelo')\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(prueba_y_real, label=\"Real\")\n",
    "    plt.plot(prueba_y_predicted, label=\"Predicción\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Train/Test Dataset\")\n",
    "    \n",
    "    ## Métricas\n",
    "    \n",
    "    rmse = mean_squared_error(y_test_real, y_test_predicted_rnn,squared=False)\n",
    "    mape = mean_absolute_percentage_error(y_test_real, y_test_predicted_rnn)\n",
    "    accuracy = 1 - mape\n",
    "    print('Acurrancy Data Test:',accuracy) #0.92\n",
    "    print('rmse Data Test :',rmse)\n",
    "    return(model,scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a62ccd-b984-4d37-9ab0-7f60717bd642",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LLamado de la función\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, scaler \u001b[38;5;241m=\u001b[39m main(\u001b[43mdf\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# LLamado de la función\n",
    "model, scaler = main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f02e3a-b3d1-454b-97c4-beece52f88b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
